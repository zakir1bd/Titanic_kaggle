{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my first post on kaggle.com as part of the \"Titanic : Machine Learning from disaster\" competition. Titanic dataset allow us to work on the supervised learning, more preciously in classification problem and easily understandable to all which is used by many analyst around the world. I would like to introduce myself in kaggle community as a data analyst through Titanic dataset where I will explore my experience to build a classification model to predict binary variables. In this analysis we will use logistic regression machine learning classification algorithm that is used to predict binary variable that contain data coded as 1 (Survived, Yes, Success, etc.) and 0 (Not Survived, No, Failure, etc.). In other words, logistic regression model predict P(Y=1) as function of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Look at the big picture\n",
    "2. Get the data (loading)\n",
    "3. Exploratory data analysis to find hidden pattern inside the data\n",
    "4. Visualization provide us opportunity to gain insights into the relationship between dependent and independent variables, to spot correlation and dependencies.\n",
    "5. Build the relationship pattern between dependent variable (Survival) and Independent variables\n",
    "6. Feature engineering\n",
    "7. Prepare the machine learning algorithm\n",
    "8. Select the best model\n",
    "9. Tune the selected model to increase the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the Big Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first question to ask your business is what exactly is the business objective; building a model is probably\n",
    "not the end goal. How does the company expect to use and benefit from this model? This is important\n",
    "because it will determine how you frame the problem, what algorithms you will select, what performance\n",
    "measure you will use to evaluate your model, and how much effort you should spend tweaking it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear algebra\n",
    "import numpy as np \n",
    "\n",
    "# Data processing\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# Data Visualization\n",
    "#from PIL import  Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dataset (traning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran, Mr. James    male   NaN      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"c:\\\\Data Science\\\\Titanic_kaggle\\\\train.csv\")\n",
    "train_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data definition"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Passengerid\n",
    "Survived (1-Yes, 0-No)\n",
    "Pclass (Ticket Class)\n",
    "Name (Passengers Name)\n",
    "Sex (Male or Female)\n",
    "Age (Age in Years)\n",
    "SibSp (# of siblings /Spouse)\n",
    "Parch (# of parents/children)\n",
    "Ticket (Ticket number)\n",
    "Fare (Passenger Fare)\n",
    "Cabin ( Cabin Number)\n",
    "Embarked (Port of embarkment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data info\n",
    "By info command  we can tell the number of rows and columns, data types of the columns and if null values exist in them.\n",
    "The training-set has 891 observations and 11 features + the target variable (survived). 2 of the features are floats, 5 are integers and 5 are objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    891\n",
       "Survived         2\n",
       "Pclass           3\n",
       "Name           891\n",
       "Sex              2\n",
       "Age             88\n",
       "SibSp            7\n",
       "Parch            7\n",
       "Ticket         681\n",
       "Fare           248\n",
       "Cabin          147\n",
       "Embarked         3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The df.unique() command allows us to better understand what does each column mean. Looking at the Survived and Sex columns, they only have 2 unique values. It usually means that they are categorical columns, in this case, it should be True or False for Survived and Male or Female for Sex.\n",
    "\n",
    "We can also observe other categorical columns like Embarked, Pclass and more. We can’t really tell what does Pclass stand for, let’s explore more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From summary statistics we can find that survival rate was 38%, so it is not imbalance data. \n",
    "Passengers age range (0.4 to 80) and average age was 29. \n",
    "We also detect some fearure missing value i.e. age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  Title  \n",
       "0      0         A/5 21171   7.2500   NaN        S      1  \n",
       "1      0          PC 17599  71.2833   C85        C      3  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S      2  \n",
       "3      0            113803  53.1000  C123        S      3  \n",
       "4      0            373450   8.0500   NaN        S      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [train_df]\n",
    "titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "\n",
    "for dataset in data:\n",
    "    # extract titles\n",
    "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    # replace titles with a more common title or as Rare\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n",
    "                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "    # convert titles into numbers\n",
    "    dataset['Title'] = dataset['Title'].map(titles)\n",
    "    # filling NaN with 0, to get safe\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "###train_df = train_df.drop(['Name'], axis=1)\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Group Bucket for Age attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tenure to categorical column\n",
    "def age_group(train_df) :\n",
    "    \n",
    "    if train_df[\"Age\"] <= 10 :\n",
    "        return \"1_10\"\n",
    "    elif (train_df[\"Age\"] > 10) & (train_df[\"Age\"] <= 20 ):\n",
    "        return \"11_20\"\n",
    "    elif (train_df[\"Age\"] > 20) & (train_df[\"Age\"] <= 30) :\n",
    "           return \"21_30\"\n",
    "    elif (train_df[\"Age\"] > 30) & (train_df[\"Age\"] <= 40) :\n",
    "           return \"31_40\"\n",
    "    elif (train_df[\"Age\"] > 40) & (train_df[\"Age\"] <= 50) :\n",
    "           return \"41_50\"\n",
    "           \n",
    "    elif (train_df[\"Age\"] > 50) & (train_df[\"Age\"] <= 60) :\n",
    "           return \"51_60\"\n",
    "    elif train_df[\"Age\"] > 60 :\n",
    "           return \"Over_60\"\n",
    "    \n",
    "train_df[\"Age_group\"] = train_df.apply(lambda train_df:age_group(train_df), axis = 1)\n",
    "\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fare Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tenure to categorical column\n",
    "def Fare_group(train_df) :\n",
    "    \n",
    "    if train_df[\"Fare\"] <= 20 :\n",
    "        return \"3\"\n",
    "    elif (train_df[\"Fare\"] > 20) & (train_df[\"Fare\"] <= 50 ):\n",
    "        return \"2\"\n",
    "    elif (train_df[\"Fare\"] > 50) :\n",
    "           return \"1\"\n",
    "    \n",
    "    \n",
    "train_df[\"Fare_group\"] = train_df.apply(lambda train_df:Fare_group(train_df), axis = 1)\n",
    "\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Fare_group.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df.groupby(['Age_group','Survived']).count()\n",
    "#pd.crosstab([df.sex,df.survived],df.pclass,margins=True).style.background_gradient(cmap='summer_r')\n",
    "#pd.crosstab([train_df.Sex,train_df.Survived]).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have did analysis of each individual variable and checking if there is any outlier values present.¶ Outliers can be defined as values out of range [(Q1-1.5IQR) , (Q3+1.5IQR)] but here I choose a range based on Maximum and Minimum value for each variable selected by observing Boxplot of corresponding variable. After identify, Outlier values will be imputed by \"mean\" of respective variable by implementation of following \"impute_outliers\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "    \n",
    "    \n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        \n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "    # select observations containing more than 2 outliers\n",
    "        outlier_indices = Counter(outlier_indices)        \n",
    "        multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "    \n",
    "    return multiple_outliers   \n",
    "\n",
    "# detect outliers from Age, SibSp , Parch and Fare\n",
    "Outliers_to_drop = detect_outliers(train_df,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\n",
    "train_df.loc[Outliers_to_drop] # Show the outliers rows\n",
    "    # Drop outliers\n",
    "train_df = train_df.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\n",
    "train_df.info()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = train_df.quantile(0.25)\n",
    "Q3 = train_df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[~((train_df < (Q1 - 1.5 * IQR)) |(train_df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_out.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.boxplot(x='Pclass',y='Age',hue = 'Survived', data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Survived\", data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "sns.boxplot('Age', data=train_df)\n",
    "##sns.boxplot(x='Pclass',y='Age',hue = 'Survived', data=train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#axis = 0 means vertically\n",
    "train_df.apply(lambda x: sum(x.isnull()), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more details about missing value\n",
    "total = train_df.isnull().sum().sort_values(ascending=False)\n",
    "percent_1 = train_df.isnull().sum()/train_df.isnull().count()*100\n",
    "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
    "missing_data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Target variable - Survival Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Survival Rate\n",
    "f,ax=plt.subplots(1,2,figsize=(10,4))\n",
    "train_df['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\n",
    "ax[0].set_title('Survived')\n",
    "ax[0].set_ylabel('')\n",
    "sns.countplot('Survived',data=train_df,ax=ax[1])\n",
    "ax[1].set_title('Survived')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we know that 38.4% was survival rate, need to go insights to understand the distribution of 38.4% passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization : Survival Vs Sex\n",
    "f,ax=plt.subplots(1,2,figsize=(10,4))\n",
    "train_df[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\n",
    "ax[0].set_title('Survived vs sex')\n",
    "sns.countplot('Sex',hue='Survived',data=train_df,ax=ax[1])\n",
    "ax[1].set_title('Survived vs Dead')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization : Passenger Class vs Survival\n",
    "f,ax=plt.subplots(1,2,figsize=(12,4))\n",
    "sns.countplot('Embarked',hue='Survived',data=train_df,ax=ax[0])\n",
    "ax[0].set_title('Embarked:Survived')\n",
    "\n",
    "sns.countplot('Pclass',hue='Survived',data=train_df,ax=ax[1])\n",
    "ax[1].set_title('Pclass: Survived')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"]= (14, 4)\n",
    "sns.countplot(x=\"Age_group\", hue=\"Survived\", data=train_df, palette=\"Set2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = {\"male\": 0, \"female\": 1}\n",
    "data = [train_df]\n",
    "\n",
    "for dataset in data:\n",
    "    dataset['Sex'] = dataset['Sex'].map(genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer \n",
    "imputer = Imputer(strategy=\"median\")\n",
    "\n",
    "tr_df_cat = train_df[[ \"Embarked\", 'Pclass',\"Age_group\"]]\n",
    "tr_df_num = train_df[[\"Survived\",\"Sex\",\"Title\",\"Age\",'Fare','Fare_group',\"SibSp\",\"Parch\"]]\n",
    "\n",
    "# for numerical missing value\n",
    "imputer.fit(tr_df_num)\n",
    "X = imputer.transform(tr_df_num)\n",
    "df_tr_num = pd.DataFrame(X, columns=tr_df_num.columns)\n",
    "\n",
    "# for categorical missing valye\n",
    "df_tr_cat = tr_df_cat.apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
    "\n",
    "\n",
    "\n",
    "cat_cols = [\"Embarked\", 'Pclass',\"Age_group\"]\n",
    "cat_processed = pd.get_dummies(tr_df_cat, prefix_sep=\"_\",columns=cat_cols)\n",
    "\n",
    "tr_df_prepared = pd.concat([df_tr_num, cat_processed], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df_prepared.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = {\"male\": 0, \"female\": 1}\n",
    "data = [train_df]\n",
    "\n",
    "for dataset in data:\n",
    "    dataset['Sex'] = dataset['Sex'].map(genders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrics\n",
    "# feature Selection- It provides score for correlation between the variables\n",
    "# How the variables are interrelated with each other\n",
    "# We should take consider these variable for prdiction where correlation matrics results are high\n",
    "\n",
    "sns.heatmap(tr_df_prepared.corr(),annot=True,cmap='RdYlGn',linewidths=0.4) #data.corr()-->correlation matrix\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(20,12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = tr_df_prepared.corr()\n",
    "corr_matrix[\"Survived\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, all features are not necessary to build the classification model. Only we need to consider best possible independent variables which are are strongly related to churn target variable. We should select the features based on importance of the correlation metrics.\n",
    "\n",
    "According to correlation matrix we choose seven metrics these are positively related to churn (i.e. Contract_Month_to_Month, \n",
    "tenure_group_Tenure_0-12, InternetService_Fibre optic, PaymentMethod_Elctronic check, MonthlyCharges, PaperlessBilling and \n",
    "Senior Citizen etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting dataset into training (80%) and testing (20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df_prepared.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target \n",
    "y = tr_df_prepared['Survived']\n",
    "# Independent variable\n",
    "X = tr_df_prepared[['Sex','Title', 'Pclass_1','Embarked_C', 'Age_group_1_10']]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the Input (X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg = log_reg.fit(X_train, y_train)\n",
    "\n",
    "Train_model_score = round(log_reg.score(X_train,y_train)*100,2)\n",
    "print(\"Train Model Score : \" ,Train_model_score)\n",
    "\n",
    "# cross validattion check\n",
    "validation_score = cross_val_score(log_reg, X_train, y_train, cv=5, scoring =\"accuracy\")\n",
    "print(\"Cross Validation Score: \", validation_score)\n",
    "print(\"Mean validation score: \", validation_score.mean())\n",
    "\n",
    "# Prediction\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "print(\"Test Model Score: \", accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actual_Predicted= pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})  \n",
    "Actual_Predicted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#axis = 0 means vertically\n",
    "tr_df_prepared.apply(lambda x: sum(x.isnull()), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df_prepared.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target \n",
    "y = tr_df_prepared['Survived']\n",
    "# Independent variable\n",
    "X = tr_df_prepared[['Sex','Title', 'Pclass_1','Fare','Embarked_C','Age_group_1_10','Age_group_11_20','Pclass_2','Pclass_3','Embarked_S' ]]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier( n_estimators=300, max_depth=4, bootstrap=True, max_features = 'auto', random_state=123)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "Y_prediction = random_forest.predict(X_test)\n",
    "\n",
    "# cross validattion check\n",
    "validation_score = cross_val_score(random_forest, X_train, y_train, cv=5, scoring =\"accuracy\")\n",
    "print(\"Cross Validation Score: \", validation_score)\n",
    "print(\"Mean validation score: \", validation_score.mean())\n",
    "\n",
    "random_forest.score(X_train, y_train)\n",
    "acc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\n",
    "acc_random_forest\n",
    "print(\"Train Model Score: \",acc_random_forest)\n",
    "print(\"Test Model Score: \", accuracy_score(Y_prediction,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "RF = RandomForestClassifier(random_state=123)\n",
    "param_grid = [{'n_estimators':  [4, 5, 10, 20, 50,200]}]\n",
    "\n",
    "grid_search_RF = GridSearchCV(RF, param_grid, cv=10,scoring='roc_auc')\n",
    "grid_search_RF.fit(X_train, y_train)\n",
    "\n",
    "Y_prediction1 = grid_search_RF.predict(X_test)\n",
    "print(\"Test Model Score: \", accuracy_score(Y_prediction1,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres_RF = grid_search_RF.cv_results_\n",
    "\n",
    "for mean_score, params in zip(cvres_RF[\"mean_test_score\"], cvres_RF[\"params\"]):\n",
    "    print(mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_RF.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"c:\\Data Science\\Kaggle\\Titanic\\Test.csv\")\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [test_df]\n",
    "titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "\n",
    "for dataset in data1:\n",
    "    # extract titles\n",
    "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    # replace titles with a more common title or as Rare\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n",
    "                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "    # convert titles into numbers\n",
    "    dataset['Title'] = dataset['Title'].map(titles)\n",
    "    # filling NaN with 0, to get safe\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "###train_df = train_df.drop(['Name'], axis=1)\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tenure to categorical column\n",
    "def age_group(test_df) :\n",
    "    \n",
    "    if test_df[\"Age\"] <= 10 :\n",
    "        return \"1_10\"\n",
    "    elif (test_df[\"Age\"] > 10) & (test_df[\"Age\"] <= 20 ):\n",
    "        return \"11_20\"\n",
    "    elif (test_df[\"Age\"] > 20) & (test_df[\"Age\"] <= 30) :\n",
    "           return \"21_30\"\n",
    "    elif (test_df[\"Age\"] > 30) & (test_df[\"Age\"] <= 40) :\n",
    "           return \"31_40\"\n",
    "    elif (test_df[\"Age\"] > 40) & (test_df[\"Age\"] <= 50) :\n",
    "           return \"41_50\"\n",
    "           \n",
    "    elif (test_df[\"Age\"] > 50) & (test_df[\"Age\"] <= 60) :\n",
    "           return \"51_60\"\n",
    "    elif test_df[\"Age\"] > 60 :\n",
    "           return \"Over_60\"\n",
    "    \n",
    "test_df[\"Age_group\"] = test_df.apply(lambda test_df:age_group(test_df), axis = 1)\n",
    "\n",
    "genders = {\"male\": 0, \"female\": 1}\n",
    "data = [test_df]\n",
    "\n",
    "for dataset in data:\n",
    "    dataset['Sex'] = dataset['Sex'].map(genders)\n",
    "\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tenure to categorical column\n",
    "def Fare_group(test_df) :\n",
    "    \n",
    "    if test_df[\"Fare\"] <= 20 :\n",
    "        return \"3\"\n",
    "    elif (test_df[\"Fare\"] > 20) & (test_df[\"Fare\"] <= 50 ):\n",
    "        return \"2\"\n",
    "    elif (test_df[\"Fare\"] > 50) :\n",
    "           return \"1\"\n",
    "    \n",
    "    \n",
    "test_df[\"Fare_group\"] = test_df.apply(lambda test_df:Fare_group(test_df), axis = 1)\n",
    "\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer \n",
    "imputer = Imputer(strategy=\"median\")\n",
    "\n",
    "te_df_cat = test_df[[ \"Embarked\", 'Pclass',\"Age_group\"]]\n",
    "te_df_num = test_df[['Title','Age',\"Sex\",\"SibSp\",\"Parch\",\"Fare\", 'Fare_group']]\n",
    "\n",
    "# for numerical missing value\n",
    "imputer.fit(te_df_num)\n",
    "X = imputer.transform(te_df_num)\n",
    "df_te_num = pd.DataFrame(X, columns=te_df_num.columns)\n",
    "\n",
    "# for categorical missing valye\n",
    "df_te_cat = te_df_cat.apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
    "\n",
    "\n",
    "\n",
    "cat_cols = [\"Embarked\", 'Pclass',\"Age_group\"]\n",
    "cat_processed = pd.get_dummies(te_df_cat, prefix_sep=\"_\",columns=cat_cols)\n",
    "\n",
    "te_df_prepared = pd.concat([df_te_num, cat_processed], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation : Confusion metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#axis = 0 means vertically\n",
    "\n",
    "final_test.apply(lambda x: sum(x.isnull()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = te_df_prepared[['Sex','Title', 'Pclass_1','Fare','Embarked_C','Age_group_1_10','Age_group_11_20' ,'Pclass_2', 'Pclass_3','Embarked_S','Fare_group' ]]\n",
    "final_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction = random_forest.predict(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "        \"Survived\": final_prediction\n",
    "    })\n",
    "submission.to_csv(\"c:\\Data Science\\Kaggle\\Titanic\\zaa6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much better way to evaluate the performance of a classifier is to look at the confusion matrix. The\n",
    "general idea is to count the number of times instances of class A are classified as class B. For example, to\n",
    "5th know the number of times the classifier confused images of 5s with 3s, you would look in the row and\n",
    "3rd column of the confusion matrix.\n",
    "\n",
    "To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to\n",
    "the actual targets. You could make predictions on the test set, but let’s keep it untouched for now\n",
    "(remember that you want to use the test set only at the very end of your project, once you have a classifier\n",
    "that you are ready to launch). Instead, you can use the function:cross_val_predict()\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
    "\n",
    "Just like the function, performs K-fold cross-validation, cross_val_score() cross_val_predict()\n",
    "but instead of returning the evaluation scores, it returns the predictions made on each test fold. This means\n",
    "that you get a clean prediction for each instance in the training set (“clean” meaning that the prediction is\n",
    "made by a model that n ever saw the data during training). Now you are ready to get the confusion matrix using the\n",
    "function. Just pass it the confusion_matrix()target classes (5) and the predicted classes (y_train) predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_train, y_pred)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actual_Predicted= pd.DataFrame({'Actual': y_train, 'Predicted': y_pred})  \n",
    "Actual_Predicted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "tree_cm = metrics.confusion_matrix( y_train, y_pred, [1,0] )\n",
    "\n",
    "sns.heatmap(tree_cm, annot=True,  fmt='.2f', xticklabels = [\"Positive\", \"Negative\"] , yticklabels = [\"Positive\", \"Negative\"] )\n",
    "plt.ylabel('Actual (True Positive)')\n",
    "plt.xlabel('Predicted (False Positive)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = TP (True positive)+ TN (True Negative)/Total (total number of classifier)\n",
    "\n",
    "Recall(True Positive Rate): When it's actually yes, how often does it predict yes?\n",
    "Recall = TP/TP+FN 618/(618+840) = 42%\n",
    "\n",
    "Precision: When it predicts yes, how often is it correct?\n",
    "Precision : TP/(TP+FP) = 618/(618+359) = 63%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_score(y_train, y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often convenient to combine precision and recall into a single metric called the F1\n",
    "score, in particular if you need a simple way to compare two classifiers. The\n",
    "F1score is the harmonic mean of precision and recall. Whereas the regular mean treats all values equally, the harmonic\n",
    "mean gives much more weight to low values. As a result, the classifier will only get a high F1 score if\n",
    "both recall and precision are high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers.\n",
    "It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC\n",
    "curve plots the true positive rate (another name for recall) against the false positive rate.\n",
    "\n",
    "To plot the ROC curve, you first need to compute the TPR and FPR for various threshold values, using the\n",
    "function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "#fpr, tpr, thresholds = roc_curve(y_test, log_reg.predict_proba(X_train)[:,1])  \n",
    "## fpr, tpr, thresholds = roc_curve( X_train, y_score)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, log_reg.predict_proba(X_test)[:,1])\n",
    "\n",
    "# Calculate the AUC \n",
    "roc_auc = auc(fpr, tpr) \n",
    "print('ROC AUC: %0.2f' % roc_auc) \n",
    "   \n",
    "# Plot of a ROC curve for a specific class \n",
    "plt.figure() \n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc) \n",
    "plt.plot([0, 1], [0, 1], 'k--') \n",
    "plt.xlim([0.0, 1.0]) \n",
    "plt.ylim([0.0, 1.05]) \n",
    "plt.xlabel('False Positive Rate') \n",
    "plt.ylabel('True Positive Rate') \n",
    "plt.title('ROC Curve') \n",
    "plt.legend(loc=\"lower right\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again there is a tradeoff: the higher the recall (TPR), the more false positives (FPR) the classifier\n",
    "produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays\n",
    "as far away from that line as possible (toward the top-left corner).\n",
    "One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will\n",
    "have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5.\n",
    "\n",
    "ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#! pip install graphviz\n",
    "#! pip install pydotplus\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import tree\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG,display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth = 3,\n",
    "                                           splitter  = \"best\",\n",
    "                                           criterion = \"gini\")\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "features = X.columns\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True, feature_names = features, class_names=['Not Churn','Churn'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('diabetes.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "dt_cm = metrics.confusion_matrix( y_test, y_test_pred, [1,0] )\n",
    "\n",
    "sns.heatmap(dt_cm, annot=True,  fmt='.2f', xticklabels = [\"Positive\", \"Negative\"] , yticklabels = [\"Positive\", \"Negative\"] )\n",
    "plt.ylabel('Actual (True Positive)')\n",
    "plt.xlabel('Predicted (False Positive)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(max_depth = 3, min_samples_split=2, n_estimators =100, random_state = 1)\n",
    "\n",
    "RF= forest.fit(X_train,y_train)\n",
    "RF_Pred = RF.predict(X_test)\n",
    "\n",
    "Model_forest = round(RF.score(X_train,y_train)*100,2)\n",
    "print(\"Training Model Score : \" , Model_forest)\n",
    "Acc_forest = metrics.accuracy_score( y_test, RF_Pred)\n",
    "print(\"Acc_Score : \", Acc_forest)\n",
    "\n",
    "#Confusion Metrix \n",
    "\n",
    "\n",
    "cnf_metrix = (metrics.confusion_matrix(y_test,RF_Pred))\n",
    "cmap = sns.cubehelix_palette(50, hue=0.5, rot=0, light=0.9, dark=0, as_cmap=True)\n",
    "sns.heatmap(cnf_metrix,cmap = cmap,xticklabels=['0','1'],yticklabels=['0','1'],annot=True, fmt=\"d\",)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, RF_Pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "#gives model report in dataframe\n",
    "def model_report(model,X_train,X_test,y_train,y_test,name) :\n",
    "    model.fit(X_train,y_train)\n",
    "    predictions  = model.predict(X_test)\n",
    "    accuracy     = accuracy_score(y_test,predictions)\n",
    "    recallscore  = recall_score(y_test,predictions)\n",
    "    precision    = precision_score(y_test,predictions)\n",
    "    roc_auc      = roc_auc_score(y_test,predictions)\n",
    "    f1score      = f1_score(y_test,predictions) \n",
    "    #kappa_metric = cohen_kappa_score(y_test,predictions)\n",
    "    \n",
    "    df = pd.DataFrame({\"Model\"           : [name],\n",
    "                       \"Accuracy_score\"  : [accuracy],\n",
    "                       \"Recall_score\"    : [recallscore],\n",
    "                       \"Precision\"       : [precision],\n",
    "                       \"f1_score\"        : [f1score],\n",
    "                       \"Area_under_curve\": [roc_auc],\n",
    "                     #  \"Kappa_metric\"    : [kappa_metric],\n",
    "                      })\n",
    "    return df\n",
    "\n",
    "#outputs for every model\n",
    "model1 = model_report(log_reg,X_train,X_test,y_train,y_train,\n",
    "                      \"Logistic Regression(Baseline_model)\")\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(max_depth = 9,\n",
    "                                       random_state = 123,\n",
    "                                       splitter  = \"best\",\n",
    "                                       criterion = \"gini\",\n",
    "                                      )\n",
    "model4 = model_report(decision_tree,X_train,X_test,y_train,y_train,\n",
    "                      \"Decision Tree\")\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators = 1000,\n",
    "                             random_state = 123,\n",
    "                             max_depth = 9,\n",
    "                             criterion = \"gini\")\n",
    "model6 = model_report(X_train,X_test,y_train,y_train,\n",
    "                      \"Random Forest Classifier\")\n",
    "\n",
    "#concat all models\n",
    "model_performances = pd.concat([model1,\n",
    "                                model4,model6,],axis = 0).reset_index()\n",
    "\n",
    "model_performances = model_performances.drop(columns = \"index\",axis =1)\n",
    "\n",
    "table  = ff.create_table(np.round(model_performances,4))\n",
    "\n",
    "py.iplot(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "RF = RandomForestClassifier(random_state=123)\n",
    "param_grid = [{'n_estimators':  [4, 5, 10, 20, 50]}]\n",
    "\n",
    "grid_search_RF = GridSearchCV(RF, param_grid, cv=5 ,scoring='roc_auc')\n",
    "grid_search_RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_RF.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres_RF = grid_search_RF.cv_results_\n",
    "\n",
    "for mean_score, params in zip(cvres_RF[\"mean_test_score\"], cvres_RF[\"params\"]):\n",
    "    print(mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
